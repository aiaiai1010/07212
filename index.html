<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>姿勢＋深度検出 完全版（深度表示あり）</title>
  <style>
    body {
      font-size: 20px;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      height: 100vh;
      margin: 0;
      text-align: center;
    }
    #content {
      display: flex;
      gap: 10px;
      margin-top: 10px;
    }
    video, canvas {
      width: 360px;
      height: 640px;
      border: 1px solid #aaa;
      transform: scaleX(-1);
    }
    #output {
      white-space: pre-line;
      margin: 10px;
    }
    #warning {
      font-size: 24px;
      font-weight: bold;
    }
  </style>
</head>
<body>
  <h1>姿勢＋深度検出 完全版（深度表示あり）</h1>
  <button id="startBtn">開始</button>
  <div id="output">準備中...</div>
  <div id="warning"></div>
  <div id="content">
    <video id="video" autoplay playsinline muted></video>
    <canvas id="canvas"></canvas>
    <canvas id="depthCanvas"></canvas>
  </div>

  <!-- ライブラリ読み込み -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/body-segmentation"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/depth-estimation"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>

  <script type="module">
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const depthCanvas = document.getElementById('depthCanvas');
    const ctx = canvas.getContext('2d');
    const dctx = depthCanvas.getContext('2d');
    const out = document.getElementById('output');
    const warn = document.getElementById('warning');
    let pitch = 0, roll = 0;
    let estimator;
    let lastTimestamp = 0;
    let processing = false;

    window.addEventListener('deviceorientation', e => {
      pitch = e.beta ?? 0;
      roll = e.gamma ?? 0;
    });

    async function initDepth() {
      await tf.setBackend('webgl');
      await tf.ready();
      return await depthEstimation.createEstimator(
        depthEstimation.SupportedModels.ARPortraitDepth
      );
    }

    const faceMesh = new FaceMesh({
      locateFile: f => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${f}`
    });

    faceMesh.setOptions({
      maxNumFaces: 1,
      refineLandmarks: true,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.3
    });

    faceMesh.onResults(onFace);

    async function onFace(results) {
      if (processing) return;
      processing = true;

      try {
        const now = Date.now();
        if (now - lastTimestamp < 150) {
          processing = false;
          return;
        }
        lastTimestamp = now;

        if (!results.multiFaceLandmarks || results.multiFaceLandmarks.length === 0) {
          out.textContent = `傾き: ${pitch.toFixed(1)}°, ${roll.toFixed(1)}°\n顔未検出`;
          warn.textContent = "顔が検出できません";
          warn.style.color = 'red';
          processing = false;
          return;
        }

        ctx.drawImage(results.image, 0, 0);

        const depthMap = await estimator.estimateDepth(video, { minDepth: 0, maxDepth: 1 });
        const depthTensor = depthMap.depth;

        const normalized = tf.tidy(() => {
          const min = depthTensor.min();
          const max = depthTensor.max();
          return depthTensor.sub(min).div(max.sub(min)).mul(255).toInt();
        });

        const [w, h] = depthTensor.shape;
        const depthArray = await normalized.data();

        function jetColor(v) {
          const t = v / 255;
          const r = Math.floor(255 * Math.max(Math.min(1.5 - Math.abs(4 * t - 3), 1), 0));
          const g = Math.floor(255 * Math.max(Math.min(1.5 - Math.abs(4 * t - 2), 1), 0));
          const b = Math.floor(255 * Math.max(Math.min(1.5 - Math.abs(4 * t - 1), 1), 0));
          return [r, g, b];
        }

        const imageData = dctx.createImageData(w, h);
        for (let i = 0; i < depthArray.length; i++) {
          const [r, g, b] = jetColor(depthArray[i]);
          imageData.data[i * 4 + 0] = r;
          imageData.data[i * 4 + 1] = g;
          imageData.data[i * 4 + 2] = b;
          imageData.data[i * 4 + 3] = 255;
        }

        depthCanvas.width = w;
        depthCanvas.height = h;
        dctx.putImageData(imageData, 0, 0);

        tf.dispose([depthTensor, normalized]);

        const lm = results.multiFaceLandmarks[0];
        const nose = lm[1], chin = lm[152];
        const yDiff = chin.y - nose.y;
        const dir = yDiff < 0.13 ? '下を向き' : yDiff > 0.19 ? '上を向き' : 'まっすぐ';
        const bent = pitch <= 40 ? (dir !== '上') : (dir === '下');

        const ix = Math.floor(nose.x * depthCanvas.width);
        const iy = Math.floor(nose.y * depthCanvas.height);
        const safeX = Math.max(0, Math.min(ix - 10, depthCanvas.width - 20));
        const safeY = Math.max(0, Math.min(iy - 10, depthCanvas.height - 20));
        const imgData = dctx.getImageData(safeX, safeY, 20, 20).data;

        let sum = 0, count = 0;
        for (let i = 0; i < imgData.length; i += 4) {
          sum += imgData[i];
          count++;
        }
        const avg = sum / count;

        out.textContent = 
          `Pitch: ${pitch.toFixed(1)}° Roll: ${roll.toFixed(1)}°\n顔: ${dir}\n距離指標: ${avg.toFixed(1)}`;

        let msg = bent ? '首が曲がっています\n' : '首は自然です\n';
        msg += avg < 50 ? '近すぎます' : avg > 200 ? '離れすぎています' : '適切な距離です';
        warn.textContent = msg;
        warn.style.color = (bent || avg < 50 || avg > 200) ? 'red' : 'green';

      } catch (e) {
        console.error("処理エラー:", e);
      } finally {
        processing = false;
      }
    }

    document.getElementById('startBtn').onclick = async () => {
      estimator = await initDepth();
      const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user' } });
      video.srcObject = stream;
      await new Promise(r => video.onloadedmetadata = r);
      canvas.width = depthCanvas.width = 360;
      canvas.height = depthCanvas.height = 640;

      new Camera(video, {
        onFrame: async () => await faceMesh.send({ image: video }),
        width: 640,
        height: 360
      }).start();

      document.getElementById('startBtn').style.display = 'none';
    };
  </script>
</body>
</html>
